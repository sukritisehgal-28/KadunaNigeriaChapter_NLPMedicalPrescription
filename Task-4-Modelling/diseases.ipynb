{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b3aee6-6efb-4f25-848e-e7f2cddcf334",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Not in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf16093-31fd-4047-8a74-f728e1a0d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_core_sci_sm-0.5.0.tar.gz\n",
    "# !pip show spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be7f8f3-c10c-45c2-a5da-d5e7d2605b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 10:28:12.410245: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-01 10:28:12.437408: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-01 10:28:12.872797: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import spacy\n",
    "# from PyPDF2 import PdfReader\n",
    "# from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments, pipeline\n",
    "# from datasets import load_dataset, Dataset\n",
    "# import faiss\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "# import os\n",
    "# import json\n",
    "# from multiprocessing import Pool\n",
    "# import logging\n",
    "# import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfb7c4d8-5da2-496c-a3ce-ca6d8eaa0e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SciSpaCy model...\n",
      "==== Medical NLP System with SciSpaCy and BioBERT (No PDFs) ====\n",
      "Loading CSV from merge_demo_amos_v3.csv...\n",
      "CSV loaded with 7541 rows and columns: ['Unnamed: 0', 'disease_name', 'description', 'symptoms', 'causes', 'source', 'table_name', 'treatment', 'url', 'diagnosis', 'medication_name', 'medication_url', 'dosage_url', 'prognosis', 'research', 'complications', 'faq', 'transmission', 'disease_name_standardized', 'source_standardized', 'symptoms_SciSpaCy', 'diagnosis_SciSpaCy', 'medication_SciSpaCy', 'treatment_SciSpaCy', 'treatment_BioBERT']\n",
      "Processing 2314 CSV rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSV: 100%|█████████████████████████████████████████████████████████████| 2314/2314 [00:00<00:00, 8015.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1 training examples\n",
      "Loading BioBERT model...\n",
      "Training on 0 examples, validating on 1 examples\n",
      "Saved tokenizer and model architecture (simplified training for demo)\n",
      "Setting up embedding generation...\n",
      "Generating medication embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 32.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating medication FAISS index...\n",
      "FAISS medication index created successfully\n",
      "Setting up diagnosis system...\n",
      "Diagnosis system ready\n",
      "\n",
      "--- Sample Diagnosis ---\n",
      "Analyzing prescription: Patient is taking Metformin 500mg daily\n",
      "Extracted entities: Patient, Metformin 500mg, daily\n",
      "\n",
      "Sample Diagnosis Result:\n",
      "Unable to determine a specific diagnosis for 'Patient is taking Metformin 500mg daily' based on available data.\n",
      "\n",
      "System is ready for use. You can diagnose diseases by providing prescriptions.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter prescription (or 'exit' to quit):  conditions that cause afib such as thyroid disease will be treated you may also need any of the following heart medicines help control your heart rate or rhythm you may need more than medicine to treat your symptoms antiplatelet or blood thinner medicines help prevent blood clots and stroke cardioversion is a procedure to return your heart rate and rhythm to normal it can be done using medicines or electric shock afib ablation is a procedure that uses energy to burn a small area of heart tissue this creates scar tissue and prevents electrical signals that cause afib you may need this procedure more than time ask for more information on afib ablation a pacemaker may be inserted into your heart a pacemaker is a device that controls your heartbeat a pacemaker may be inserted during an ablation procedure or surgery ask your healthcare provider for more information on pacemakers surgery may be needed if other procedures do not work during surgery your healthcare provider will make cuts in the upper part of your heart the provider will stitch the cuts together to create scar tissue the scar tissue will prevent electrical signals that cause afib the following list of medications are related to or used in the treatment of this condition xarelto metoprolol multaq coreg digoxin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing prescription: conditions that cause afib such as thyroid disease will be treated you may also need any of the following heart medicines help control your heart rate or rhythm you may need more than medicine to treat your symptoms antiplatelet or blood thinner medicines help prevent blood clots and stroke cardioversion is a procedure to return your heart rate and rhythm to normal it can be done using medicines or electric shock afib ablation is a procedure that uses energy to burn a small area of heart tissue this creates scar tissue and prevents electrical signals that cause afib you may need this procedure more than time ask for more information on afib ablation a pacemaker may be inserted into your heart a pacemaker is a device that controls your heartbeat a pacemaker may be inserted during an ablation procedure or surgery ask your healthcare provider for more information on pacemakers surgery may be needed if other procedures do not work during surgery your healthcare provider will make cuts in the upper part of your heart the provider will stitch the cuts together to create scar tissue the scar tissue will prevent electrical signals that cause afib the following list of medications are related to or used in the treatment of this condition xarelto metoprolol multaq coreg digoxin\n",
      "Extracted entities: conditions, thyroid disease, treated, heart medicines, heart rate, rhythm, medicine, treat, symptoms, antiplatelet, blood thinner medicines, blood clots, stroke, cardioversion, return your, heart rate, rhythm, medicines, electric shock afib, ablation\n",
      "\n",
      "Diagnosis:\n",
      "Unable to determine a specific diagnosis for 'conditions that cause afib such as thyroid disease will be treated you may also need any of the following heart medicines help control your heart rate or rhythm you may need more than medicine to treat your symptoms antiplatelet or blood thinner medicines help prevent blood clots and stroke cardioversion is a procedure to return your heart rate and rhythm to normal it can be done using medicines or electric shock afib ablation is a procedure that uses energy to burn a small area of heart tissue this creates scar tissue and prevents electrical signals that cause afib you may need this procedure more than time ask for more information on afib ablation a pacemaker may be inserted into your heart a pacemaker is a device that controls your heartbeat a pacemaker may be inserted during an ablation procedure or surgery ask your healthcare provider for more information on pacemakers surgery may be needed if other procedures do not work during surgery your healthcare provider will make cuts in the upper part of your heart the provider will stitch the cuts together to create scar tissue the scar tissue will prevent electrical signals that cause afib the following list of medications are related to or used in the treatment of this condition xarelto metoprolol multaq coreg digoxin' based on available data.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 278\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDiagnosis:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdiagnosis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 278\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 270\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Interactive mode\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mEnter prescription (or \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m to quit): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import spacy\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import faiss\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "# import os\n",
    "# import json\n",
    "# import logging\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# print(\"Loading SciSpaCy model...\")\n",
    "# nlp = spacy.load(\"en_core_sci_sm\", disable=[\"ner\"])  # Disable NER if only tokenization needed\n",
    "# nlp.enable_pipe(\"ner\")  # Re-enable NER only when needed\n",
    "\n",
    "# def load_csv_from_drive(csv_path):\n",
    "#     print(f\"Loading CSV from {csv_path}...\")\n",
    "    \n",
    "#     if not os.path.exists(csv_path):\n",
    "#         print(f\"Error: CSV file {csv_path} does not exist!\")\n",
    "#         return None\n",
    "    \n",
    "#     csv_data = pd.read_csv(csv_path)\n",
    "#     print(f\"CSV loaded with {len(csv_data)} rows and columns: {csv_data.columns.tolist()}\")\n",
    "\n",
    "#     # Check if required columns exist\n",
    "#     required_cols = [\"medication_name\", \"symptoms\", \"diagnosis_SciSpaCy\", \"diagnosis\"]\n",
    "#     missing_cols = [col for col in required_cols if col not in csv_data.columns]\n",
    "\n",
    "#     if missing_cols:\n",
    "#         print(f\"Warning: Missing columns: {missing_cols}\")\n",
    "#         # Create missing columns with NaN values\n",
    "#         for col in missing_cols:\n",
    "#             csv_data[col] = np.nan\n",
    "\n",
    "#     return csv_data\n",
    "\n",
    "# def process_csv_row(row):\n",
    "#     input_text = f\"Patient is taking {row['medication_name']}\"\n",
    "#     if pd.notna(row.get(\"symptoms\")):\n",
    "#         doc = nlp(str(row[\"symptoms\"])[:5000])  # Limit length\n",
    "#         symptoms_entities = \", \".join([ent.text for ent in doc.ents][:20])\n",
    "#         input_text += f\". Symptoms: {symptoms_entities}\"\n",
    "\n",
    "#     # Check which diagnosis column to use\n",
    "#     if pd.notna(row.get(\"diagnosis_SciSpaCy\")):\n",
    "#         output_text = row[\"diagnosis_SciSpaCy\"]\n",
    "#     elif pd.notna(row.get(\"diagnosis\")):\n",
    "#         output_text = row[\"diagnosis\"]\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "#     return {\"input\": input_text, \"output\": output_text}\n",
    "\n",
    "# def prepare_fine_tune_data(csv_data):\n",
    "#     csv_data = csv_data.dropna(subset=[\"medication_name\"])  # Drop early to reduce memory\n",
    "#     print(f\"Processing {len(csv_data)} CSV rows...\")\n",
    "\n",
    "#     fine_tune_data = []\n",
    "#     # Process rows directly instead of with Pool to avoid issues\n",
    "#     for _, row in tqdm(csv_data.iterrows(), total=len(csv_data), desc=\"Processing CSV\"):\n",
    "#         result = process_csv_row(row)\n",
    "#         if result is not None:\n",
    "#             fine_tune_data.append(result)\n",
    "\n",
    "#     print(f\"Created {len(fine_tune_data)} training examples\")\n",
    "\n",
    "#     with open(\"fine_tune_data_scispacy.jsonl\", \"w\") as f:\n",
    "#         for item in fine_tune_data:\n",
    "#             f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "#     return fine_tune_data\n",
    "\n",
    "# def fine_tune_biobert(fine_tune_data):\n",
    "#     print(\"Loading BioBERT model...\")\n",
    "#     # Use pre-trained BERT base model instead of causal LM which was causing issues\n",
    "#     model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#     model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "#     # Create dataset directly from our data\n",
    "#     train_data = fine_tune_data[:int(len(fine_tune_data)*0.8)]\n",
    "#     val_data = fine_tune_data[int(len(fine_tune_data)*0.8):]\n",
    "\n",
    "#     print(f\"Training on {len(train_data)} examples, validating on {len(val_data)} examples\")\n",
    "\n",
    "#     # Simple embedding-based model for matching inputs to outputs\n",
    "#     class BiobertEmbeddingModel(torch.nn.Module):\n",
    "#         def __init__(self, biobert_model):\n",
    "#             super().__init__()\n",
    "#             self.biobert = biobert_model\n",
    "#             self.linear = torch.nn.Linear(768, 768)  # Assuming BERT base hidden size\n",
    "\n",
    "#         def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "#             outputs = self.biobert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             pooled_output = outputs.last_hidden_state[:, 0]  # CLS token\n",
    "#             return self.linear(pooled_output)\n",
    "\n",
    "#     # Create a simple embedding model\n",
    "#     embedding_model = BiobertEmbeddingModel(model)\n",
    "\n",
    "#     # Save the tokenizer and model architecture\n",
    "#     tokenizer.save_pretrained(\"./biobert_tokenizer\")\n",
    "#     torch.save(embedding_model.state_dict(), \"./biobert_embedding_model.pt\")\n",
    "\n",
    "#     print(\"Saved tokenizer and model architecture (simplified training for demo)\")\n",
    "#     return tokenizer, embedding_model\n",
    "\n",
    "# def create_embeddings_csv(csv_data, tokenizer, embedding_model):\n",
    "#     print(\"Setting up embedding generation...\")\n",
    "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     embedding_model = embedding_model.to(device)\n",
    "#     embedding_model.eval()\n",
    "\n",
    "#     def get_biobert_embeddings(texts, batch_size=16):\n",
    "#         embeddings = []\n",
    "#         for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "#             batch = texts[i:i+batch_size]\n",
    "#             encoded = tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "#             input_ids = encoded['input_ids'].to(device)\n",
    "#             attention_mask = encoded['attention_mask'].to(device)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = embedding_model.biobert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#                 # Use CLS token representation\n",
    "#                 batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "#                 embeddings.extend(batch_embeddings)\n",
    "\n",
    "#         return np.array(embeddings)\n",
    "\n",
    "#     print(\"Generating medication embeddings...\")\n",
    "#     medication_names = csv_data[\"medication_name\"].fillna(\"Unknown\").tolist()\n",
    "#     # Limit sample size for demonstration\n",
    "#     sample_size = min(len(medication_names), 100)\n",
    "#     medication_sample = medication_names[:sample_size]\n",
    "    \n",
    "#     # Check if medication_sample is empty\n",
    "#     if len(medication_sample) == 0:\n",
    "#         print(\"Warning: No medication names found in the CSV\")\n",
    "#         return np.array([]), medication_sample\n",
    "    \n",
    "#     medication_embeddings = get_biobert_embeddings(medication_sample)\n",
    "    \n",
    "#     return medication_embeddings, medication_sample\n",
    "\n",
    "# def create_faiss_index_csv(medication_embeddings):\n",
    "#     print(\"Creating medication FAISS index...\")\n",
    "    \n",
    "#     if medication_embeddings.size == 0:\n",
    "#         print(\"Warning: No embeddings to index\")\n",
    "#         return None\n",
    "        \n",
    "#     dimension = medication_embeddings.shape[1]\n",
    "#     medication_index = faiss.IndexFlatL2(dimension)\n",
    "#     medication_index.add(medication_embeddings)\n",
    "    \n",
    "#     print(\"FAISS medication index created successfully\")\n",
    "#     return medication_index\n",
    "\n",
    "# def setup_diagnosis_system(csv_data, medication_index, tokenizer, embedding_model, medication_sample):\n",
    "#     print(\"Setting up diagnosis system...\")\n",
    "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     embedding_model = embedding_model.to(device)\n",
    "#     embedding_model.eval()\n",
    "\n",
    "#     def get_embedding(text):\n",
    "#         encoded = tokenizer([text], return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "#         input_ids = encoded['input_ids'].to(device)\n",
    "#         attention_mask = encoded['attention_mask'].to(device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = embedding_model.biobert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "#         return embedding\n",
    "\n",
    "#     def diagnose_disease(prescription):\n",
    "#         print(f\"Analyzing prescription: {prescription}\")\n",
    "\n",
    "#         # Extract entities\n",
    "#         doc = nlp(prescription[:5000])\n",
    "#         entities = \", \".join([ent.text for ent in doc.ents][:20])\n",
    "#         enriched_input = f\"{prescription}. Entities: {entities}\"\n",
    "#         print(f\"Extracted entities: {entities}\")\n",
    "\n",
    "#         # Get embedding\n",
    "#         query_embedding = get_embedding(enriched_input)\n",
    "\n",
    "#         # Check if medication_index exists\n",
    "#         if medication_index is None:\n",
    "#             return \"Unable to analyze prescription: No medication data available.\"\n",
    "\n",
    "#         # Search medication index\n",
    "#         try:\n",
    "#             D_med, I_med = medication_index.search(query_embedding, k=2)\n",
    "#             if I_med.shape[0] > 0 and I_med.shape[1] > 0:\n",
    "#                 similar_medications = [medication_sample[i] for i in I_med[0] if i < len(medication_sample)]\n",
    "#             else:\n",
    "#                 similar_medications = []\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error in medication search: {e}\")\n",
    "#             similar_medications = []\n",
    "\n",
    "#         # Find potential diagnoses from similar medications\n",
    "#         potential_diagnoses = []\n",
    "#         for med in similar_medications:\n",
    "#             matches = csv_data[csv_data['medication_name'] == med]\n",
    "#             if not matches.empty:\n",
    "#                 for _, row in matches.iterrows():\n",
    "#                     if pd.notna(row.get('diagnosis')):\n",
    "#                         potential_diagnoses.append(row['diagnosis'])\n",
    "#                     elif pd.notna(row.get('diagnosis_SciSpaCy')):\n",
    "#                         potential_diagnoses.append(row['diagnosis_SciSpaCy'])\n",
    "\n",
    "#         # Build diagnosis\n",
    "#         if potential_diagnoses:\n",
    "#             diagnosis = f\"Based on the prescription '{prescription}', the most likely diagnosis is: {potential_diagnoses[0]}\"\n",
    "#             if len(potential_diagnoses) > 1:\n",
    "#                 diagnosis += f\"\\nOther possible diagnoses: {', '.join(potential_diagnoses[1:])}\"\n",
    "#         else:\n",
    "#             diagnosis = f\"Unable to determine a specific diagnosis for '{prescription}' based on available data.\"\n",
    "\n",
    "#         return diagnosis\n",
    "\n",
    "#     print(\"Diagnosis system ready\")\n",
    "#     return diagnose_disease\n",
    "\n",
    "# # Main execution function\n",
    "# def main():\n",
    "#     print(\"==== Medical NLP System with SciSpaCy and BioBERT (No PDFs) ====\")\n",
    "\n",
    "#     # Specify path to CSV in Google Drive\n",
    "#     csv_path = 'merge_demo_amos_v3.csv'  # Update this path\n",
    "\n",
    "#     # Load CSV data from Google Drive\n",
    "#     csv_data = load_csv_from_drive(csv_path)\n",
    "    \n",
    "#     if csv_data is None:\n",
    "#         print(\"Error loading CSV data. Exiting.\")\n",
    "#         return\n",
    "\n",
    "#     # Prepare training data\n",
    "#     fine_tune_data = prepare_fine_tune_data(csv_data)\n",
    "\n",
    "#     # Setup tokenizer and model\n",
    "#     tokenizer, embedding_model = fine_tune_biobert(fine_tune_data)\n",
    "\n",
    "#     # Generate embeddings for medication data\n",
    "#     medication_embeddings, medication_sample = create_embeddings_csv(csv_data, tokenizer, embedding_model)\n",
    "\n",
    "#     # Create FAISS index for medications\n",
    "#     medication_index = create_faiss_index_csv(medication_embeddings)\n",
    "\n",
    "#     # Setup diagnosis function\n",
    "#     diagnose_disease = setup_diagnosis_system(\n",
    "#         csv_data, medication_index, tokenizer, embedding_model, medication_sample\n",
    "#     )\n",
    "\n",
    "#     # Example diagnosis\n",
    "#     print(\"\\n--- Sample Diagnosis ---\")\n",
    "#     prescription = \"Patient is taking Metformin 500mg daily\"\n",
    "#     result = diagnose_disease(prescription)\n",
    "#     print(f\"\\nSample Diagnosis Result:\\n{result}\")\n",
    "\n",
    "#     print(\"\\nSystem is ready for use. You can diagnose diseases by providing prescriptions.\")\n",
    "\n",
    "#     # Interactive mode\n",
    "#     while True:\n",
    "#         user_input = input(\"\\nEnter prescription (or 'exit' to quit): \")\n",
    "#         if user_input.lower() == 'exit':\n",
    "#             break\n",
    "\n",
    "#         diagnosis = diagnose_disease(user_input)\n",
    "#         print(f\"\\nDiagnosis:\\n{diagnosis}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b4ac96-e1e1-4b34-9934-0867866ebd0c",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f995f1e3-09b9-4e0f-b335-c1068bda5c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 15:02:49.209341: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-08 15:02:49.236891: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-08 15:02:49.834628: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV with 1318 rows\n",
      "Dataset after cleaning: 1318 rows\n",
      "Average lengths - Symptoms: 49.2, Disease: 20.7, Treatment: 62.1\n",
      "Using device: cuda\n",
      "Initialized GanjinZero/biobart-v2-base\n",
      "Train size: 1054, Eval size: 264\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saurabh/.local/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/saurabh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4109: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='330' max='330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [330/330 03:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.378041</td>\n",
       "      <td>0.044186</td>\n",
       "      <td>0.346518</td>\n",
       "      <td>0.090076</td>\n",
       "      <td>0.333695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.091000</td>\n",
       "      <td>1.203485</td>\n",
       "      <td>0.067337</td>\n",
       "      <td>0.390379</td>\n",
       "      <td>0.129102</td>\n",
       "      <td>0.374719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.091000</td>\n",
       "      <td>1.146376</td>\n",
       "      <td>0.092835</td>\n",
       "      <td>0.406607</td>\n",
       "      <td>0.154802</td>\n",
       "      <td>0.390324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.143100</td>\n",
       "      <td>1.106375</td>\n",
       "      <td>0.101020</td>\n",
       "      <td>0.421353</td>\n",
       "      <td>0.169578</td>\n",
       "      <td>0.403320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>1.091572</td>\n",
       "      <td>0.105167</td>\n",
       "      <td>0.425665</td>\n",
       "      <td>0.173274</td>\n",
       "      <td>0.405111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw pred_ids shape: (264, 39), type: <class 'numpy.ndarray'>\n",
      "Max raw pred_id: 85258, Min raw pred_id: -100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saurabh/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/home/saurabh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4109: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw pred_ids shape: (264, 42), type: <class 'numpy.ndarray'>\n",
      "Max raw pred_id: 85258, Min raw pred_id: -100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saurabh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4109: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw pred_ids shape: (264, 43), type: <class 'numpy.ndarray'>\n",
      "Max raw pred_id: 85328, Min raw pred_id: -100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saurabh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4109: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw pred_ids shape: (264, 44), type: <class 'numpy.ndarray'>\n",
      "Max raw pred_id: 85328, Min raw pred_id: -100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saurabh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4109: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/saurabh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4109: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw pred_ids shape: (264, 44), type: <class 'numpy.ndarray'>\n",
      "Max raw pred_id: 85328, Min raw pred_id: -100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "/home/saurabh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4109: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw pred_ids shape: (264, 44), type: <class 'numpy.ndarray'>\n",
      "Max raw pred_id: 85328, Min raw pred_id: -100\n",
      "Evaluation Results: {'eval_loss': 1.0915718078613281, 'eval_bleu': 0.10516674331808576, 'eval_rouge1': 0.42566452233779095, 'eval_rouge2': 0.1732742979633933, 'eval_rougeL': 0.40511128643722766, 'eval_runtime': 15.5673, 'eval_samples_per_second': 16.959, 'eval_steps_per_second': 2.12, 'epoch': 5.0}\n",
      "Model and configuration saved to model/medical_generation_both\n",
      "\n",
      "Testing model with sample inputs:\n",
      "\n",
      "Input symptoms: Wrinkled abdomen, urinary issues, undescended testes.\n",
      "Actual disease: PruneBelly Syndrome\n",
      "Actual treatment: Surgery (reconstruction), antibiotics (e.g., amoxicillin), support.\n",
      "Generated output: Diagnosis: Klinefelter Syndrome (Entry 2) Treatment: Testosterone gel (0.5% cream), surgery if small.\n",
      "\n",
      "Input symptoms: Heartburn, regurgitation, chest pain, dysphagia.\n",
      "Actual disease: Hiatal Hernia\n",
      "Actual treatment: Omeprazole (2040 mg daily), surgery (fundoplication), lifestyle changes.\n",
      "Generated output: Diagnosis: Gastroesophageal Reflux Disease (GERD) Treatment: Omeprazole (2040 mg daily), lifestyle changes, PPI.\n",
      "\n",
      "Input symptoms: Small bumps, itching, clustered growths, discomfort.\n",
      "Actual disease: Condyloma (Genital Warts)\n",
      "Actual treatment: Imiquimod (5% cream), cryotherapy, podophyllotoxin (0.5%).\n",
      "Generated output: Diagnosis: Epidermolysis Bullosa Treatment: Clobetasol (0.5% cream), treat cause (e.g., antibiotics).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "433"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import gc\n",
    "import json\n",
    "\n",
    "# Define file paths\n",
    "DATA_PATH = \"processed_diseases-583.csv\"\n",
    "MODEL_SAVE_PATH = \"model\"\n",
    "\n",
    "# PyTorch optimizations\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Load CSV\n",
    "try:\n",
    "    df = pd.read_csv(\n",
    "        DATA_PATH,\n",
    "        usecols=[\"Disease\", \"Treatment\", \"Symptoms\"],\n",
    "        engine=\"pyarrow\",\n",
    "        dtype_backend=\"numpy_nullable\",\n",
    "        dtype={\"Disease\": \"string\", \"Treatment\": \"string\", \"Symptoms\": \"string\"}\n",
    "    ).rename(columns={\"Disease\": \"disease_name\", \"Treatment\": \"treatment\", \"Symptoms\": \"symptoms\"})\n",
    "    print(f\"Loaded CSV with {len(df)} rows\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error loading CSV: {e}\")\n",
    "\n",
    "# Verify columns\n",
    "required_cols = [\"symptoms\", \"disease_name\", \"treatment\"]\n",
    "if not all(col in df.columns for col in required_cols):\n",
    "    raise ValueError(f\"Missing columns: {[col for col in required_cols if col not in df.columns]}\")\n",
    "\n",
    "# Clean NaN values and prepare data\n",
    "df[\"symptoms\"] = df[\"symptoms\"].fillna(\"\").str.strip()\n",
    "df[\"disease_name\"] = df[\"disease_name\"].fillna(\"\").str.strip()\n",
    "df[\"treatment\"] = df[\"treatment\"].fillna(\"\").str.strip()\n",
    "\n",
    "# Remove rows with empty symptoms or both empty disease and treatment\n",
    "df = df[(df[\"symptoms\"] != \"\") & ((df[\"disease_name\"] != \"\") | (df[\"treatment\"] != \"\"))]\n",
    "print(f\"Dataset after cleaning: {len(df)} rows\")\n",
    "\n",
    "# Check for data statistics\n",
    "avg_symptom_len = df[\"symptoms\"].str.len().mean()\n",
    "avg_disease_len = df[\"disease_name\"].str.len().mean()\n",
    "avg_treatment_len = df[\"treatment\"].str.len().mean()\n",
    "print(f\"Average lengths - Symptoms: {avg_symptom_len:.1f}, Disease: {avg_disease_len:.1f}, Treatment: {avg_treatment_len:.1f}\")\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# For a generation task, we'll use a seq2seq model (e.g., T5 or BART)\n",
    "try:\n",
    "    # Using a biomedical seq2seq model suitable for generation tasks\n",
    "    model_name = \"GanjinZero/biobart-v2-base\"  # Biomedical BART model for generation\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float32\n",
    "    ).to(device)\n",
    "    print(f\"Initialized {model_name}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Memory error initializing model: {e}\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    raise\n",
    "\n",
    "# Custom Dataset for seq2seq task\n",
    "class MedicalGenerationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_input_length=512, max_target_length=128, task_type=\"both\"):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "        self.task_type = task_type  # 'diagnosis', 'treatment', or 'both'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        symptoms = row[\"symptoms\"].strip()\n",
    "        \n",
    "        # Prepare input based on task\n",
    "        if self.task_type == \"diagnosis\":\n",
    "            input_text = f\"Generate a diagnosis based on these symptoms: {symptoms}\"\n",
    "            target_text = row[\"disease_name\"].strip()\n",
    "        elif self.task_type == \"treatment\":\n",
    "            input_text = f\"Generate a treatment plan based on these symptoms: {symptoms}\"\n",
    "            target_text = row[\"treatment\"].strip()\n",
    "        else:  # both\n",
    "            disease = row[\"disease_name\"].strip()\n",
    "            input_text = f\"Generate a diagnosis and treatment plan based on these symptoms: {symptoms}\"\n",
    "            treatment = row[\"treatment\"].strip()\n",
    "            target_text = f\"Diagnosis: {disease} Treatment: {treatment}\"\n",
    "        \n",
    "        # Skip examples with empty targets\n",
    "        if not target_text.strip():\n",
    "            target_text = \"No information available\"\n",
    "        \n",
    "        # Tokenize inputs and targets\n",
    "        model_inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_input_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(\n",
    "                target_text,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_target_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        input_ids = model_inputs[\"input_ids\"].squeeze()\n",
    "        attention_mask = model_inputs[\"attention_mask\"].squeeze()\n",
    "        labels = labels[\"input_ids\"].squeeze()\n",
    "        \n",
    "        # Replace padding token id with -100 for loss calculation\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# Choose task type - can be \"diagnosis\", \"treatment\", or \"both\"\n",
    "TASK_TYPE = \"both\"\n",
    "\n",
    "# Prepare datasets\n",
    "train_df, eval_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"Train size: {len(train_df)}, Eval size: {len(eval_df)}\")\n",
    "\n",
    "train_dataset = MedicalGenerationDataset(train_df, tokenizer, task_type=TASK_TYPE)\n",
    "eval_dataset = MedicalGenerationDataset(eval_df, tokenizer, task_type=TASK_TYPE)\n",
    "\n",
    "# Custom metrics for generation evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "    \n",
    "    # Replace -100 with pad token id\n",
    "    pred_mask = labels_ids != -100\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    \n",
    "    # Ensure pred_ids is a list of lists of integers\n",
    "    # This fixes the overflow error\n",
    "    if hasattr(pred_ids, \"cpu\"):\n",
    "        pred_ids = pred_ids.cpu().numpy()\n",
    "    \n",
    "    # Ensure all values are within valid integer range\n",
    "    # This is a more robust approach for handling the token IDs\n",
    "    \n",
    "    # First, log the range of values\n",
    "    print(f\"Raw pred_ids shape: {pred_ids.shape}, type: {type(pred_ids)}\")\n",
    "    if hasattr(pred_ids, \"size\"):  # Check if it's a numpy array\n",
    "        print(f\"Max raw pred_id: {np.max(pred_ids)}, Min raw pred_id: {np.min(pred_ids)}\")\n",
    "    \n",
    "    # Make sure we have integer values\n",
    "    if np.issubdtype(pred_ids.dtype, np.floating):\n",
    "        print(\"Converting floating point predictions to integers\")\n",
    "        pred_ids = np.rint(pred_ids).astype(np.int64)\n",
    "    \n",
    "    # Use a safer approach: process batch item by item\n",
    "    pred_str = []\n",
    "    for i in range(len(pred_ids)):\n",
    "        try:\n",
    "            # Get a single item from the batch\n",
    "            item_ids = pred_ids[i].tolist() if hasattr(pred_ids[i], \"tolist\") else list(pred_ids[i])\n",
    "            \n",
    "            # Ensure all IDs are valid Python integers\n",
    "            item_ids = [int(id) for id in item_ids if id != -100]\n",
    "            \n",
    "            # Filter out any potentially problematic token IDs (very large values)\n",
    "            item_ids = [id for id in item_ids if id < tokenizer.vocab_size]\n",
    "            \n",
    "            # Decode a single item\n",
    "            decoded = tokenizer.decode(item_ids, skip_special_tokens=True)\n",
    "            pred_str.append(decoded)\n",
    "        except Exception as e:\n",
    "            print(f\"Error decoding item {i}: {e}\")\n",
    "            # Fallback to an empty string if decoding fails\n",
    "            pred_str.append(\"\")\n",
    "    \n",
    "    # Process labels similarly\n",
    "    label_str = []\n",
    "    for i in range(len(labels_ids)):\n",
    "        try:\n",
    "            # Get a single item\n",
    "            item_ids = labels_ids[i].tolist() if hasattr(labels_ids[i], \"tolist\") else list(labels_ids[i])\n",
    "            \n",
    "            # Ensure all IDs are valid Python integers\n",
    "            item_ids = [int(id) for id in item_ids if id != -100]\n",
    "            \n",
    "            # Decode a single item\n",
    "            decoded = tokenizer.decode(item_ids, skip_special_tokens=True)\n",
    "            label_str.append(decoded)\n",
    "        except Exception as e:\n",
    "            print(f\"Error decoding label {i}: {e}\")\n",
    "            label_str.append(\"\")\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {\"rouge1\": 0, \"rouge2\": 0, \"rougeL\": 0}\n",
    "    \n",
    "    # Calculate BLEU and ROUGE for each prediction\n",
    "    bleu_scores = []\n",
    "    \n",
    "    for pred, label in zip(pred_str, label_str):\n",
    "        # Skip empty predictions or labels\n",
    "        if not pred.strip() or not label.strip():\n",
    "            continue\n",
    "            \n",
    "        # BLEU score with smoothing\n",
    "        try:\n",
    "            pred_tokens = pred.split()\n",
    "            label_tokens = label.split()\n",
    "            smoothie = SmoothingFunction().method1\n",
    "            bleu = sentence_bleu([label_tokens], pred_tokens, smoothing_function=smoothie)\n",
    "            bleu_scores.append(bleu)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating BLEU: {e}\")\n",
    "            \n",
    "        # ROUGE scores\n",
    "        try:\n",
    "            scores = scorer.score(label, pred)\n",
    "            rouge_scores[\"rouge1\"] += scores[\"rouge1\"].fmeasure\n",
    "            rouge_scores[\"rouge2\"] += scores[\"rouge2\"].fmeasure\n",
    "            rouge_scores[\"rougeL\"] += scores[\"rougeL\"].fmeasure\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating ROUGE: {e}\")\n",
    "    \n",
    "    # Calculate averages\n",
    "    n_samples = len(pred_str)\n",
    "    if n_samples > 0:\n",
    "        rouge_scores[\"rouge1\"] /= n_samples\n",
    "        rouge_scores[\"rouge2\"] /= n_samples\n",
    "        rouge_scores[\"rougeL\"] /= n_samples\n",
    "    \n",
    "    # Average BLEU score\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
    "    \n",
    "    # Return all metrics\n",
    "    metrics = {\n",
    "        \"bleu\": avg_bleu,\n",
    "        \"rouge1\": rouge_scores[\"rouge1\"],\n",
    "        \"rouge2\": rouge_scores[\"rouge2\"],\n",
    "        \"rougeL\": rouge_scores[\"rougeL\"]\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=os.path.join(MODEL_SAVE_PATH, f\"results_{TASK_TYPE}\"),\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=os.path.join(MODEL_SAVE_PATH, \"logs\"),\n",
    "    logging_steps=100,\n",
    "    save_total_limit=3,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=4,\n",
    "    fp16=False,  # Avoid FP16 issues\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=5e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge2\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "try:\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    eval_results = trainer.evaluate()\n",
    "    print(\"Evaluation Results:\", eval_results)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    raise\n",
    "\n",
    "# Save model\n",
    "try:\n",
    "    model_save_path = os.path.join(MODEL_SAVE_PATH, f\"medical_generation_{TASK_TYPE}\")\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    \n",
    "    # Save training configuration\n",
    "    config = {\n",
    "        \"task_type\": TASK_TYPE,\n",
    "        \"model_name\": model_name,\n",
    "        \"dataset_size\": len(df),\n",
    "        \"train_size\": len(train_df),\n",
    "        \"eval_size\": len(eval_df),\n",
    "        \"metrics\": eval_results\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(model_save_path, \"training_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(f\"Model and configuration saved to {model_save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")\n",
    "\n",
    "# Test the model with sample inputs\n",
    "def generate_medical_output(symptoms, task_type=TASK_TYPE):\n",
    "    # Prepare input based on task\n",
    "    if task_type == \"diagnosis\":\n",
    "        input_text = f\"Generate a diagnosis based on these symptoms: {symptoms}\"\n",
    "    elif task_type == \"treatment\":\n",
    "        input_text = f\"Generate a treatment plan based on these symptoms: {symptoms}\"\n",
    "    else:  # both\n",
    "        input_text = f\"Generate a diagnosis and treatment plan based on these symptoms: {symptoms}\"\n",
    "    \n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate output\n",
    "    try:\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        # Handle token ID decoding in a more robust way\n",
    "        try:\n",
    "            # First attempt: direct decoding if possible\n",
    "            decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in direct decoding: {e}\")\n",
    "            \n",
    "            # Second attempt: Convert to Python list and filter\n",
    "            try:\n",
    "                # Convert to CPU and numpy if needed\n",
    "                if hasattr(outputs, \"cpu\"):\n",
    "                    outputs_list = outputs[0].cpu().tolist()\n",
    "                else:\n",
    "                    outputs_list = outputs[0].tolist() if hasattr(outputs[0], \"tolist\") else list(outputs[0])\n",
    "                \n",
    "                # Filter out any potentially problematic token IDs\n",
    "                outputs_list = [int(id) for id in outputs_list if id < tokenizer.vocab_size]\n",
    "                \n",
    "                # Try decoding the filtered list\n",
    "                decoded = tokenizer.decode(outputs_list, skip_special_tokens=True)\n",
    "                print(\"Successfully decoded after filtering\")\n",
    "            except Exception as e2:\n",
    "                print(f\"Error in fallback decoding: {e2}\")\n",
    "                decoded = \"Error: Could not decode model output\"\n",
    "            \n",
    "        return decoded\n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation: {e}\")\n",
    "        return \"Error generating output\"\n",
    "\n",
    "# Sample test with a few examples from the test set\n",
    "if len(eval_df) > 0:\n",
    "    print(\"\\nTesting model with sample inputs:\")\n",
    "    \n",
    "    # Select a few random samples from eval set\n",
    "    sample_indices = np.random.choice(len(eval_df), min(3, len(eval_df)), replace=False)\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        sample = eval_df.iloc[idx]\n",
    "        symptoms = sample[\"symptoms\"]\n",
    "        actual_disease = sample[\"disease_name\"]\n",
    "        actual_treatment = sample[\"treatment\"]\n",
    "        \n",
    "        print(f\"\\nInput symptoms: {symptoms}\")\n",
    "        print(f\"Actual disease: {actual_disease}\")\n",
    "        print(f\"Actual treatment: {actual_treatment}\")\n",
    "        \n",
    "        # Generate output\n",
    "        generated_output = generate_medical_output(symptoms)\n",
    "        print(f\"Generated output: {generated_output}\")\n",
    "\n",
    "# Clean up\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "429ad37e-f682-4eb0-ad5f-b0e3b54b8b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Diagnosis & Treatment: Diagnosis: COVID19 (Coronavirus Disease 2019) Treatment: Oseltamivir (75 mg twice daily), oxygen, rest.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Define model path and device\n",
    "MODEL_SAVE_PATH = \"model/medical_generation_both/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_SAVE_PATH)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_SAVE_PATH).to(device)\n",
    "\n",
    "# Function to generate diagnosis and treatment\n",
    "def generate_medical_advice(symptoms, max_length=128):\n",
    "    \"\"\"Generates diagnosis and treatment based on symptoms.\"\"\"\n",
    "    \n",
    "    # Prepare input\n",
    "    input_text = f\"Generate a diagnosis and treatment plan based on these symptoms: {symptoms}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    \n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(**inputs, max_length=max_length, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    # Decode output\n",
    "    generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    test_symptoms = \"Fever, cough, difficulty breathing\"\n",
    "    result = generate_medical_advice(test_symptoms)\n",
    "    print(\"Generated Diagnosis & Treatment:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1ca8be-b93b-4a0f-8ba5-4469a55c4844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
