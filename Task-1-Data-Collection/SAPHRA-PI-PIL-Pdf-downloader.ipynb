{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, StaleElementReferenceException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_folder = \"downloaded_pdfs\"\n",
    "os.makedirs(download_folder, exist_ok=True)\n",
    "\n",
    "# Track downloaded PDF URLs to avoid duplicates\n",
    "downloaded_pdfs = set()\n",
    "\n",
    "# Setup Chrome options (headless for speed)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.set_page_load_timeout(100) \n",
    "\n",
    "# Load the target URL\n",
    "driver.get(\"https://pi-pil-repository.sahpra.org.za/\")\n",
    "\n",
    "# Use a persistent requests session for downloads\n",
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_page_load(driver, timeout=60):\n",
    "    \"\"\"Wait until the document is fully loaded.\"\"\"\n",
    "    try:\n",
    "        WebDriverWait(driver, timeout).until(\n",
    "            lambda d: d.execute_script(\"return document.readyState\") == \"complete\"\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        print(\"‚ö†Ô∏è Timeout waiting for the page to fully load.\")\n",
    "\n",
    "def wait_for_pdf_links(driver, timeout=60):\n",
    "    \"\"\"\n",
    "    Wait for at least one PDF link (<a> ending with '.pdf') to appear in the table.\n",
    "    Returns the list of elements once found.\n",
    "    \"\"\"\n",
    "    end_time = time.time() + timeout\n",
    "    while time.time() < end_time:\n",
    "        pdf_elements = driver.find_elements(By.CSS_SELECTOR, \"table tr td a[href$='.pdf']\")\n",
    "        if pdf_elements:\n",
    "            return pdf_elements\n",
    "        time.sleep(1)\n",
    "    raise TimeoutException(\"Timeout waiting for PDF links.\")\n",
    "\n",
    "def sanitize_filename(text):\n",
    "    \"\"\"\n",
    "    Improved sanitization:\n",
    "      - Trims whitespace.\n",
    "      - Replaces all whitespace with a single underscore.\n",
    "      - Removes characters other than alphanumeric, underscore, or hyphen.\n",
    "      - Replaces multiple underscores with a single underscore.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', '_', text)\n",
    "    text = re.sub(r'[^\\w\\-]', '', text)\n",
    "    text = re.sub(r'_+', '_', text)\n",
    "    return text\n",
    "\n",
    "def determine_file_type(link):\n",
    "    \"\"\"\n",
    "    Determine if the PDF link is for PI or PIL.\n",
    "    First, check the link's visible text.\n",
    "    If inconclusive, inspect the href (filename) for common substrings.\n",
    "    \"\"\"\n",
    "    link_text = link.text.strip().lower()\n",
    "    href = link.get_attribute(\"href\").lower()\n",
    "    \n",
    "    # Check the link's text first.\n",
    "    if \"pil\" in link_text:\n",
    "        return \"pil\"\n",
    "    elif \"pi\" in link_text:\n",
    "        return \"pi\"\n",
    "    \n",
    "    # Fallback: check the filename in the URL.\n",
    "    filename = href.split(\"/\")[-1]\n",
    "    if \"epil\" in filename or \"-pil\" in filename:\n",
    "        return \"pil\"\n",
    "    elif \"epi\" in filename or \"-pi\" in filename:\n",
    "        return \"pi\"\n",
    "    \n",
    "    # Last resort:\n",
    "    if \"pil\" in href:\n",
    "        return \"pil\"\n",
    "    elif \"pi\" in href:\n",
    "        return \"pi\"\n",
    "    \n",
    "    return \"unknown\"\n",
    "\n",
    "def download_pdf(pdf_url, pdf_filename):\n",
    "    \"\"\"Download a PDF from the given URL to the specified filename.\"\"\"\n",
    "    if pdf_url in downloaded_pdfs:\n",
    "        return None\n",
    "    downloaded_pdfs.add(pdf_url)\n",
    "    try:\n",
    "        response = session.get(pdf_url, stream=True, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        with open(pdf_filename, \"wb\") as pdf_file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    pdf_file.write(chunk)\n",
    "        return pdf_filename\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {pdf_url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop: Iterate Through Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_page = 1\n",
    "max_retries = 2  # Number of refresh attempts per page\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    while True:\n",
    "        print(f\"\\nüìÑ Processing Page {current_page}...\")\n",
    "\n",
    "        # Ensure the page is fully loaded before proceeding\n",
    "        wait_for_page_load(driver, timeout=60)\n",
    "\n",
    "        # Try to wait for PDF links with a retry mechanism\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                _ = wait_for_pdf_links(driver, timeout=60)\n",
    "                break  # Found PDF links‚Äîexit the retry loop\n",
    "            except TimeoutException:\n",
    "                retries += 1\n",
    "                print(f\"‚ö†Ô∏è Timeout waiting for PDF links on page {current_page}. Refreshing (attempt {retries})...\")\n",
    "                driver.refresh()\n",
    "                wait_for_page_load(driver, timeout=60)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Failed to load PDF links on page {current_page} after {max_retries} attempts. Skipping page...\")\n",
    "            current_page += 1\n",
    "            continue\n",
    "\n",
    "        # -------------------------------\n",
    "        # Process Each Table Row for Product Name and PDF links\n",
    "        # -------------------------------\n",
    "        pdf_info = []  # List of tuples: (pdf_url, desired_file_path)\n",
    "        # Get all rows once; then process them by index so we can re-fetch individual rows if needed.\n",
    "        rows = driver.find_elements(By.CSS_SELECTOR, \"table tr\")\n",
    "        for i in range(len(rows)):\n",
    "            for attempt in range(3):\n",
    "                try:\n",
    "                    # Re-fetch the row by index on each attempt\n",
    "                    row = driver.find_elements(By.CSS_SELECTOR, \"table tr\")[i]\n",
    "                    try:\n",
    "                        product_name = row.find_element(By.CSS_SELECTOR, \"td:nth-child(1)\").text.strip()\n",
    "                        if not product_name:\n",
    "                            product_name = \"unknown\"\n",
    "                    except Exception:\n",
    "                        product_name = \"unknown\"\n",
    "                    product_name = sanitize_filename(product_name)\n",
    "                    \n",
    "                    # Get all PDF links in this row\n",
    "                    pdf_links = row.find_elements(By.CSS_SELECTOR, \"td a[href$='.pdf']\")\n",
    "                    \n",
    "                    if len(pdf_links) == 2:\n",
    "                        # If exactly two PDF links exist, assume the first is PI and the second is PIL.\n",
    "                        pi_url = pdf_links[0].get_attribute(\"href\")\n",
    "                        pil_url = pdf_links[1].get_attribute(\"href\")\n",
    "                        file_name_pi = f\"{product_name}-pi.pdf\"\n",
    "                        file_name_pil = f\"{product_name}-pil.pdf\"\n",
    "                        full_file_path_pi = os.path.join(download_folder, file_name_pi)\n",
    "                        full_file_path_pil = os.path.join(download_folder, file_name_pil)\n",
    "                        pdf_info.append((pi_url, full_file_path_pi))\n",
    "                        pdf_info.append((pil_url, full_file_path_pil))\n",
    "                    else:\n",
    "                        # Otherwise, determine the file type for each link.\n",
    "                        for link in pdf_links:\n",
    "                            try:\n",
    "                                pdf_url = link.get_attribute(\"href\")\n",
    "                                if not pdf_url or pdf_url in downloaded_pdfs:\n",
    "                                    continue\n",
    "\n",
    "                                file_type = determine_file_type(link)\n",
    "                                if file_type not in [\"pi\", \"pil\"]:\n",
    "                                    print(f\"‚ö†Ô∏è Unable to determine file type for URL: {pdf_url}\")\n",
    "                                    continue\n",
    "\n",
    "                                file_name = f\"{product_name}-{file_type}.pdf\"\n",
    "                                full_file_path = os.path.join(download_folder, file_name)\n",
    "                                pdf_info.append((pdf_url, full_file_path))\n",
    "                            except StaleElementReferenceException:\n",
    "                                # If a link becomes stale, let the outer loop re-fetch the row.\n",
    "                                raise\n",
    "                    break  # Successfully processed this row; exit the retry loop.\n",
    "                except StaleElementReferenceException:\n",
    "                    if attempt < 2:\n",
    "                        print(f\"‚ö†Ô∏è Stale element caught for row {i}. Retrying (attempt {attempt + 1})...\")\n",
    "                        time.sleep(1)\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è Skipping row {i} after repeated stale element errors.\")\n",
    "\n",
    "\n",
    "        # -------------------------------\n",
    "        # Download PDFs concurrently\n",
    "        # -------------------------------\n",
    "        if pdf_info:\n",
    "            futures = {\n",
    "                executor.submit(download_pdf, url, filename): (url, filename)\n",
    "                for (url, filename) in pdf_info\n",
    "            }\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    print(f\"Downloaded: {result}\")\n",
    "\n",
    "\n",
    "        # -------------------------------\n",
    "        # Navigate to the Next Page\n",
    "        # -------------------------------\n",
    "        try:\n",
    "            pagination = driver.find_element(By.CSS_SELECTOR, \"ul.pagination\")\n",
    "            next_button = pagination.find_element(By.XPATH, \".//li/a[contains(text(),'‚Ä∫')]\")\n",
    "            # Check if the next button is disabled (e.g., via a 'disabled' class)\n",
    "            if \"disabled\" in next_button.get_attribute(\"class\"):\n",
    "                print(\"No more pages to process.\")\n",
    "                break\n",
    "\n",
    "            # Click the next button using JavaScript\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            wait_for_page_load(driver, timeout=60)\n",
    "            time.sleep(2)  # Give a moment for asynchronous content to load\n",
    "            current_page += 1\n",
    "\n",
    "        except (NoSuchElementException, TimeoutException) as e:\n",
    "            print(\"'Next' button not found or page did not load in time, stopping...\")\n",
    "            break\n",
    "\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpinmed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
